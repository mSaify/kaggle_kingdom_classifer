{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attempt_hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Po1Sw32_-aOA",
        "sghgvGZKKd60"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1q8V5OT-IKs"
      },
      "source": [
        "import csv\n",
        "from random import randrange\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUAngdRZJmsY"
      },
      "source": [
        "train = pd.read_csv(\"Train.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3yJBZNjJuL4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "83669db3-e0ce-41e1-becd-7ac66af043b0"
      },
      "source": [
        "train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kingdom</th>\n",
              "      <th>Ncodons</th>\n",
              "      <th>UUU</th>\n",
              "      <th>UUC</th>\n",
              "      <th>UUA</th>\n",
              "      <th>UUG</th>\n",
              "      <th>CUU</th>\n",
              "      <th>CUC</th>\n",
              "      <th>CUA</th>\n",
              "      <th>CUG</th>\n",
              "      <th>AUU</th>\n",
              "      <th>AUC</th>\n",
              "      <th>AUA</th>\n",
              "      <th>AUG</th>\n",
              "      <th>GUU</th>\n",
              "      <th>GUC</th>\n",
              "      <th>GUA</th>\n",
              "      <th>GUG</th>\n",
              "      <th>GCU</th>\n",
              "      <th>GCC</th>\n",
              "      <th>GCA</th>\n",
              "      <th>GCG</th>\n",
              "      <th>CCU</th>\n",
              "      <th>CCC</th>\n",
              "      <th>CCA</th>\n",
              "      <th>CCG</th>\n",
              "      <th>UGG</th>\n",
              "      <th>GGU</th>\n",
              "      <th>GGC</th>\n",
              "      <th>GGA</th>\n",
              "      <th>GGG</th>\n",
              "      <th>UCU</th>\n",
              "      <th>UCC</th>\n",
              "      <th>UCA</th>\n",
              "      <th>UCG</th>\n",
              "      <th>AGU</th>\n",
              "      <th>AGC</th>\n",
              "      <th>ACU</th>\n",
              "      <th>ACC</th>\n",
              "      <th>ACA</th>\n",
              "      <th>ACG</th>\n",
              "      <th>UAU</th>\n",
              "      <th>UAC</th>\n",
              "      <th>CAA</th>\n",
              "      <th>CAG</th>\n",
              "      <th>AAU</th>\n",
              "      <th>AAC</th>\n",
              "      <th>UGU</th>\n",
              "      <th>UGC</th>\n",
              "      <th>CAU</th>\n",
              "      <th>CAC</th>\n",
              "      <th>AAA</th>\n",
              "      <th>AAG</th>\n",
              "      <th>CGU</th>\n",
              "      <th>CGC</th>\n",
              "      <th>CGA</th>\n",
              "      <th>CGG</th>\n",
              "      <th>AGA</th>\n",
              "      <th>AGG</th>\n",
              "      <th>GAU</th>\n",
              "      <th>GAC</th>\n",
              "      <th>GAA</th>\n",
              "      <th>GAG</th>\n",
              "      <th>UAA</th>\n",
              "      <th>UAG</th>\n",
              "      <th>UGA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rod</td>\n",
              "      <td>1139</td>\n",
              "      <td>0.043869</td>\n",
              "      <td>0.035073</td>\n",
              "      <td>0.042635</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.041777</td>\n",
              "      <td>0.032421</td>\n",
              "      <td>0.049396</td>\n",
              "      <td>0.007110</td>\n",
              "      <td>0.065917</td>\n",
              "      <td>0.046055</td>\n",
              "      <td>0.043550</td>\n",
              "      <td>0.006566</td>\n",
              "      <td>0.024868</td>\n",
              "      <td>0.009354</td>\n",
              "      <td>0.016534</td>\n",
              "      <td>0.004032</td>\n",
              "      <td>0.016706</td>\n",
              "      <td>0.024469</td>\n",
              "      <td>0.021511</td>\n",
              "      <td>0.009982</td>\n",
              "      <td>0.031805</td>\n",
              "      <td>0.019998</td>\n",
              "      <td>0.014429</td>\n",
              "      <td>0.012112</td>\n",
              "      <td>0.003553</td>\n",
              "      <td>0.013465</td>\n",
              "      <td>0.021215</td>\n",
              "      <td>0.038418</td>\n",
              "      <td>0.006901</td>\n",
              "      <td>0.016078</td>\n",
              "      <td>0.006382</td>\n",
              "      <td>0.040576</td>\n",
              "      <td>0.007292</td>\n",
              "      <td>0.008472</td>\n",
              "      <td>0.009022</td>\n",
              "      <td>0.017210</td>\n",
              "      <td>0.028778</td>\n",
              "      <td>0.037423</td>\n",
              "      <td>0.006782</td>\n",
              "      <td>0.028112</td>\n",
              "      <td>0.022912</td>\n",
              "      <td>0.019141</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.022385</td>\n",
              "      <td>0.024059</td>\n",
              "      <td>0.013935</td>\n",
              "      <td>0.010187</td>\n",
              "      <td>0.014958</td>\n",
              "      <td>0.030018</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.009450</td>\n",
              "      <td>0.008019</td>\n",
              "      <td>0.007330</td>\n",
              "      <td>0.017310</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.004311</td>\n",
              "      <td>0.023807</td>\n",
              "      <td>0.011808</td>\n",
              "      <td>0.017619</td>\n",
              "      <td>0.007039</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.001189</td>\n",
              "      <td>0.029038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vrt</td>\n",
              "      <td>1012</td>\n",
              "      <td>0.013118</td>\n",
              "      <td>0.057591</td>\n",
              "      <td>0.022825</td>\n",
              "      <td>0.006641</td>\n",
              "      <td>0.025885</td>\n",
              "      <td>0.054591</td>\n",
              "      <td>0.095176</td>\n",
              "      <td>0.019620</td>\n",
              "      <td>0.023399</td>\n",
              "      <td>0.062736</td>\n",
              "      <td>0.033893</td>\n",
              "      <td>0.012634</td>\n",
              "      <td>0.007702</td>\n",
              "      <td>0.024325</td>\n",
              "      <td>0.016733</td>\n",
              "      <td>0.007503</td>\n",
              "      <td>0.013331</td>\n",
              "      <td>0.043944</td>\n",
              "      <td>0.031553</td>\n",
              "      <td>0.007485</td>\n",
              "      <td>0.008834</td>\n",
              "      <td>0.034751</td>\n",
              "      <td>0.033154</td>\n",
              "      <td>0.002121</td>\n",
              "      <td>0.004874</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.024340</td>\n",
              "      <td>0.020616</td>\n",
              "      <td>0.014323</td>\n",
              "      <td>0.012755</td>\n",
              "      <td>0.030900</td>\n",
              "      <td>0.025705</td>\n",
              "      <td>0.003340</td>\n",
              "      <td>0.002980</td>\n",
              "      <td>0.009815</td>\n",
              "      <td>0.027256</td>\n",
              "      <td>0.046073</td>\n",
              "      <td>0.049161</td>\n",
              "      <td>0.015857</td>\n",
              "      <td>0.004117</td>\n",
              "      <td>0.018044</td>\n",
              "      <td>0.028931</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.010981</td>\n",
              "      <td>0.036167</td>\n",
              "      <td>0.003762</td>\n",
              "      <td>0.015372</td>\n",
              "      <td>0.003436</td>\n",
              "      <td>0.022869</td>\n",
              "      <td>0.032610</td>\n",
              "      <td>0.003807</td>\n",
              "      <td>0.008335</td>\n",
              "      <td>0.012871</td>\n",
              "      <td>0.012357</td>\n",
              "      <td>0.009790</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.007703</td>\n",
              "      <td>0.011224</td>\n",
              "      <td>0.015875</td>\n",
              "      <td>0.021296</td>\n",
              "      <td>0.001194</td>\n",
              "      <td>0.011095</td>\n",
              "      <td>0.004745</td>\n",
              "      <td>0.027767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vrl</td>\n",
              "      <td>1422</td>\n",
              "      <td>0.022753</td>\n",
              "      <td>0.024898</td>\n",
              "      <td>0.018039</td>\n",
              "      <td>0.027769</td>\n",
              "      <td>0.015621</td>\n",
              "      <td>0.017421</td>\n",
              "      <td>0.018036</td>\n",
              "      <td>0.017666</td>\n",
              "      <td>0.020898</td>\n",
              "      <td>0.024449</td>\n",
              "      <td>0.026945</td>\n",
              "      <td>0.035908</td>\n",
              "      <td>0.021845</td>\n",
              "      <td>0.020784</td>\n",
              "      <td>0.022888</td>\n",
              "      <td>0.028211</td>\n",
              "      <td>0.027254</td>\n",
              "      <td>0.013666</td>\n",
              "      <td>0.015743</td>\n",
              "      <td>0.022096</td>\n",
              "      <td>0.010105</td>\n",
              "      <td>0.010882</td>\n",
              "      <td>0.023443</td>\n",
              "      <td>0.016296</td>\n",
              "      <td>0.006551</td>\n",
              "      <td>0.028780</td>\n",
              "      <td>0.016369</td>\n",
              "      <td>0.025429</td>\n",
              "      <td>0.010628</td>\n",
              "      <td>0.014161</td>\n",
              "      <td>0.015318</td>\n",
              "      <td>0.015918</td>\n",
              "      <td>0.021844</td>\n",
              "      <td>0.016076</td>\n",
              "      <td>0.023455</td>\n",
              "      <td>0.025627</td>\n",
              "      <td>0.027996</td>\n",
              "      <td>0.017787</td>\n",
              "      <td>0.023861</td>\n",
              "      <td>0.021627</td>\n",
              "      <td>0.040316</td>\n",
              "      <td>0.019388</td>\n",
              "      <td>0.007835</td>\n",
              "      <td>0.029524</td>\n",
              "      <td>0.026432</td>\n",
              "      <td>0.016785</td>\n",
              "      <td>0.004851</td>\n",
              "      <td>0.015581</td>\n",
              "      <td>0.015275</td>\n",
              "      <td>0.027512</td>\n",
              "      <td>0.026689</td>\n",
              "      <td>0.016643</td>\n",
              "      <td>0.011712</td>\n",
              "      <td>0.014440</td>\n",
              "      <td>0.004222</td>\n",
              "      <td>0.016720</td>\n",
              "      <td>0.011105</td>\n",
              "      <td>0.028581</td>\n",
              "      <td>0.033062</td>\n",
              "      <td>0.032497</td>\n",
              "      <td>0.028295</td>\n",
              "      <td>0.004289</td>\n",
              "      <td>0.008880</td>\n",
              "      <td>0.004675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bct</td>\n",
              "      <td>1150215</td>\n",
              "      <td>0.012533</td>\n",
              "      <td>0.042499</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.010396</td>\n",
              "      <td>0.014764</td>\n",
              "      <td>0.043486</td>\n",
              "      <td>0.004440</td>\n",
              "      <td>0.045591</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>0.048955</td>\n",
              "      <td>0.006030</td>\n",
              "      <td>0.027335</td>\n",
              "      <td>0.009229</td>\n",
              "      <td>0.045752</td>\n",
              "      <td>0.010378</td>\n",
              "      <td>0.031397</td>\n",
              "      <td>0.010282</td>\n",
              "      <td>0.053835</td>\n",
              "      <td>0.016107</td>\n",
              "      <td>0.046562</td>\n",
              "      <td>0.006155</td>\n",
              "      <td>0.019669</td>\n",
              "      <td>0.004589</td>\n",
              "      <td>0.033741</td>\n",
              "      <td>0.014747</td>\n",
              "      <td>0.011308</td>\n",
              "      <td>0.060035</td>\n",
              "      <td>0.016808</td>\n",
              "      <td>0.014470</td>\n",
              "      <td>0.019526</td>\n",
              "      <td>0.016757</td>\n",
              "      <td>0.003776</td>\n",
              "      <td>0.021557</td>\n",
              "      <td>0.009253</td>\n",
              "      <td>0.017204</td>\n",
              "      <td>0.002920</td>\n",
              "      <td>0.027845</td>\n",
              "      <td>0.008217</td>\n",
              "      <td>0.027873</td>\n",
              "      <td>0.013167</td>\n",
              "      <td>0.013926</td>\n",
              "      <td>0.009634</td>\n",
              "      <td>0.024891</td>\n",
              "      <td>0.012324</td>\n",
              "      <td>0.023328</td>\n",
              "      <td>0.005855</td>\n",
              "      <td>0.018596</td>\n",
              "      <td>0.011661</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>0.013233</td>\n",
              "      <td>0.032544</td>\n",
              "      <td>0.009205</td>\n",
              "      <td>0.042067</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.019761</td>\n",
              "      <td>0.008620</td>\n",
              "      <td>0.012742</td>\n",
              "      <td>0.020680</td>\n",
              "      <td>0.037861</td>\n",
              "      <td>0.028382</td>\n",
              "      <td>0.042827</td>\n",
              "      <td>0.001758</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.006248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vrl</td>\n",
              "      <td>3673</td>\n",
              "      <td>0.035727</td>\n",
              "      <td>0.030558</td>\n",
              "      <td>0.007909</td>\n",
              "      <td>0.035027</td>\n",
              "      <td>0.027608</td>\n",
              "      <td>0.033924</td>\n",
              "      <td>0.014007</td>\n",
              "      <td>0.022098</td>\n",
              "      <td>0.020782</td>\n",
              "      <td>0.018968</td>\n",
              "      <td>0.009662</td>\n",
              "      <td>0.020251</td>\n",
              "      <td>0.028003</td>\n",
              "      <td>0.026166</td>\n",
              "      <td>0.008566</td>\n",
              "      <td>0.033178</td>\n",
              "      <td>0.032618</td>\n",
              "      <td>0.032623</td>\n",
              "      <td>0.020462</td>\n",
              "      <td>0.015024</td>\n",
              "      <td>0.029455</td>\n",
              "      <td>0.018226</td>\n",
              "      <td>0.016676</td>\n",
              "      <td>0.014382</td>\n",
              "      <td>0.023300</td>\n",
              "      <td>0.037235</td>\n",
              "      <td>0.033505</td>\n",
              "      <td>0.012447</td>\n",
              "      <td>0.019009</td>\n",
              "      <td>0.026667</td>\n",
              "      <td>0.025664</td>\n",
              "      <td>0.016965</td>\n",
              "      <td>0.010594</td>\n",
              "      <td>0.012990</td>\n",
              "      <td>0.016507</td>\n",
              "      <td>0.018533</td>\n",
              "      <td>0.026390</td>\n",
              "      <td>0.018517</td>\n",
              "      <td>0.011232</td>\n",
              "      <td>0.012819</td>\n",
              "      <td>0.015288</td>\n",
              "      <td>0.022211</td>\n",
              "      <td>0.027916</td>\n",
              "      <td>0.011532</td>\n",
              "      <td>0.020162</td>\n",
              "      <td>0.016603</td>\n",
              "      <td>0.029207</td>\n",
              "      <td>0.031921</td>\n",
              "      <td>0.021357</td>\n",
              "      <td>0.023304</td>\n",
              "      <td>0.028013</td>\n",
              "      <td>0.011211</td>\n",
              "      <td>0.015014</td>\n",
              "      <td>0.013559</td>\n",
              "      <td>0.011595</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.010629</td>\n",
              "      <td>0.026274</td>\n",
              "      <td>0.029869</td>\n",
              "      <td>0.018175</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>0.008559</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.005837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4704</th>\n",
              "      <td>pln</td>\n",
              "      <td>16973</td>\n",
              "      <td>0.027968</td>\n",
              "      <td>0.024691</td>\n",
              "      <td>0.012946</td>\n",
              "      <td>0.024973</td>\n",
              "      <td>0.032526</td>\n",
              "      <td>0.018383</td>\n",
              "      <td>0.011836</td>\n",
              "      <td>0.020166</td>\n",
              "      <td>0.034776</td>\n",
              "      <td>0.022933</td>\n",
              "      <td>0.013739</td>\n",
              "      <td>0.025323</td>\n",
              "      <td>0.026737</td>\n",
              "      <td>0.015722</td>\n",
              "      <td>0.018394</td>\n",
              "      <td>0.017780</td>\n",
              "      <td>0.029307</td>\n",
              "      <td>0.020107</td>\n",
              "      <td>0.030311</td>\n",
              "      <td>0.008674</td>\n",
              "      <td>0.019445</td>\n",
              "      <td>0.010136</td>\n",
              "      <td>0.021030</td>\n",
              "      <td>0.010682</td>\n",
              "      <td>0.026650</td>\n",
              "      <td>0.029477</td>\n",
              "      <td>0.018756</td>\n",
              "      <td>0.033834</td>\n",
              "      <td>0.014879</td>\n",
              "      <td>0.019879</td>\n",
              "      <td>0.011887</td>\n",
              "      <td>0.019772</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.019699</td>\n",
              "      <td>0.019185</td>\n",
              "      <td>0.021371</td>\n",
              "      <td>0.013706</td>\n",
              "      <td>0.022338</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.027124</td>\n",
              "      <td>0.014798</td>\n",
              "      <td>0.021618</td>\n",
              "      <td>0.016533</td>\n",
              "      <td>0.029911</td>\n",
              "      <td>0.021069</td>\n",
              "      <td>0.014571</td>\n",
              "      <td>0.012926</td>\n",
              "      <td>0.024395</td>\n",
              "      <td>0.014893</td>\n",
              "      <td>0.031190</td>\n",
              "      <td>0.042832</td>\n",
              "      <td>0.012823</td>\n",
              "      <td>0.017731</td>\n",
              "      <td>0.006832</td>\n",
              "      <td>0.011499</td>\n",
              "      <td>0.022870</td>\n",
              "      <td>0.009828</td>\n",
              "      <td>0.034713</td>\n",
              "      <td>0.019298</td>\n",
              "      <td>0.029748</td>\n",
              "      <td>0.035063</td>\n",
              "      <td>0.003303</td>\n",
              "      <td>0.015236</td>\n",
              "      <td>0.003655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4705</th>\n",
              "      <td>bct</td>\n",
              "      <td>2224</td>\n",
              "      <td>0.044447</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>0.053739</td>\n",
              "      <td>0.019224</td>\n",
              "      <td>0.011931</td>\n",
              "      <td>0.003760</td>\n",
              "      <td>0.015186</td>\n",
              "      <td>0.004416</td>\n",
              "      <td>0.055685</td>\n",
              "      <td>0.015071</td>\n",
              "      <td>0.041673</td>\n",
              "      <td>0.023300</td>\n",
              "      <td>0.032556</td>\n",
              "      <td>0.003718</td>\n",
              "      <td>0.020535</td>\n",
              "      <td>0.007150</td>\n",
              "      <td>0.016540</td>\n",
              "      <td>0.001643</td>\n",
              "      <td>0.019001</td>\n",
              "      <td>0.009570</td>\n",
              "      <td>0.016998</td>\n",
              "      <td>0.004286</td>\n",
              "      <td>0.021381</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.006049</td>\n",
              "      <td>0.024700</td>\n",
              "      <td>0.011039</td>\n",
              "      <td>0.022762</td>\n",
              "      <td>0.006231</td>\n",
              "      <td>0.023883</td>\n",
              "      <td>0.004490</td>\n",
              "      <td>0.036080</td>\n",
              "      <td>0.001444</td>\n",
              "      <td>0.025705</td>\n",
              "      <td>0.008883</td>\n",
              "      <td>0.039007</td>\n",
              "      <td>0.010375</td>\n",
              "      <td>0.040464</td>\n",
              "      <td>0.004213</td>\n",
              "      <td>0.032286</td>\n",
              "      <td>0.007896</td>\n",
              "      <td>0.020621</td>\n",
              "      <td>0.008826</td>\n",
              "      <td>0.072327</td>\n",
              "      <td>0.021580</td>\n",
              "      <td>0.011958</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.012256</td>\n",
              "      <td>0.008385</td>\n",
              "      <td>0.115146</td>\n",
              "      <td>0.029658</td>\n",
              "      <td>0.001594</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>0.005837</td>\n",
              "      <td>0.006401</td>\n",
              "      <td>0.023190</td>\n",
              "      <td>0.003206</td>\n",
              "      <td>0.049491</td>\n",
              "      <td>0.015883</td>\n",
              "      <td>0.061619</td>\n",
              "      <td>0.016611</td>\n",
              "      <td>0.014396</td>\n",
              "      <td>0.002363</td>\n",
              "      <td>0.014104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4706</th>\n",
              "      <td>inv</td>\n",
              "      <td>1690</td>\n",
              "      <td>0.004434</td>\n",
              "      <td>0.048860</td>\n",
              "      <td>0.011386</td>\n",
              "      <td>0.009637</td>\n",
              "      <td>0.007528</td>\n",
              "      <td>0.023065</td>\n",
              "      <td>0.005046</td>\n",
              "      <td>0.073458</td>\n",
              "      <td>0.016209</td>\n",
              "      <td>0.069255</td>\n",
              "      <td>0.010451</td>\n",
              "      <td>0.038903</td>\n",
              "      <td>0.009168</td>\n",
              "      <td>0.035694</td>\n",
              "      <td>0.007611</td>\n",
              "      <td>0.034311</td>\n",
              "      <td>0.011036</td>\n",
              "      <td>0.037071</td>\n",
              "      <td>0.009853</td>\n",
              "      <td>0.030230</td>\n",
              "      <td>0.007303</td>\n",
              "      <td>0.013642</td>\n",
              "      <td>0.003873</td>\n",
              "      <td>0.027426</td>\n",
              "      <td>0.025140</td>\n",
              "      <td>0.018895</td>\n",
              "      <td>0.033380</td>\n",
              "      <td>0.015796</td>\n",
              "      <td>0.015019</td>\n",
              "      <td>0.007174</td>\n",
              "      <td>0.012729</td>\n",
              "      <td>0.005560</td>\n",
              "      <td>0.033046</td>\n",
              "      <td>0.008185</td>\n",
              "      <td>0.016011</td>\n",
              "      <td>0.002911</td>\n",
              "      <td>0.021288</td>\n",
              "      <td>0.006479</td>\n",
              "      <td>0.030963</td>\n",
              "      <td>0.014257</td>\n",
              "      <td>0.031378</td>\n",
              "      <td>0.007919</td>\n",
              "      <td>0.031229</td>\n",
              "      <td>0.008526</td>\n",
              "      <td>0.040863</td>\n",
              "      <td>0.009056</td>\n",
              "      <td>0.015390</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.021973</td>\n",
              "      <td>0.009489</td>\n",
              "      <td>0.052913</td>\n",
              "      <td>0.010147</td>\n",
              "      <td>0.026760</td>\n",
              "      <td>0.009219</td>\n",
              "      <td>0.011808</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.002308</td>\n",
              "      <td>0.024373</td>\n",
              "      <td>0.048235</td>\n",
              "      <td>0.016151</td>\n",
              "      <td>0.047535</td>\n",
              "      <td>0.006460</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.008474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4707</th>\n",
              "      <td>pln</td>\n",
              "      <td>5723</td>\n",
              "      <td>0.045390</td>\n",
              "      <td>0.022630</td>\n",
              "      <td>0.125978</td>\n",
              "      <td>0.003223</td>\n",
              "      <td>0.004213</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.005490</td>\n",
              "      <td>0.105977</td>\n",
              "      <td>0.006491</td>\n",
              "      <td>0.004662</td>\n",
              "      <td>0.034297</td>\n",
              "      <td>0.025929</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.047408</td>\n",
              "      <td>0.010737</td>\n",
              "      <td>0.036912</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>0.016083</td>\n",
              "      <td>0.011202</td>\n",
              "      <td>0.037812</td>\n",
              "      <td>0.006514</td>\n",
              "      <td>0.016922</td>\n",
              "      <td>0.008525</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.033462</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.026887</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.009550</td>\n",
              "      <td>0.008124</td>\n",
              "      <td>0.040832</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.013410</td>\n",
              "      <td>0.008321</td>\n",
              "      <td>0.027557</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.030609</td>\n",
              "      <td>0.001877</td>\n",
              "      <td>0.053411</td>\n",
              "      <td>0.004585</td>\n",
              "      <td>0.032474</td>\n",
              "      <td>0.004854</td>\n",
              "      <td>0.065198</td>\n",
              "      <td>0.005380</td>\n",
              "      <td>0.016792</td>\n",
              "      <td>0.003634</td>\n",
              "      <td>0.021925</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.038648</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>0.003879</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>0.006160</td>\n",
              "      <td>0.021747</td>\n",
              "      <td>0.002135</td>\n",
              "      <td>0.045074</td>\n",
              "      <td>0.006226</td>\n",
              "      <td>0.044749</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.005032</td>\n",
              "      <td>0.002087</td>\n",
              "      <td>0.021542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4708</th>\n",
              "      <td>inv</td>\n",
              "      <td>1808</td>\n",
              "      <td>0.002657</td>\n",
              "      <td>0.045442</td>\n",
              "      <td>0.015164</td>\n",
              "      <td>0.012560</td>\n",
              "      <td>0.008726</td>\n",
              "      <td>0.028971</td>\n",
              "      <td>0.013301</td>\n",
              "      <td>0.033678</td>\n",
              "      <td>0.012636</td>\n",
              "      <td>0.040714</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.034166</td>\n",
              "      <td>0.011310</td>\n",
              "      <td>0.036643</td>\n",
              "      <td>0.003122</td>\n",
              "      <td>0.022093</td>\n",
              "      <td>0.014177</td>\n",
              "      <td>0.041372</td>\n",
              "      <td>0.006115</td>\n",
              "      <td>0.025884</td>\n",
              "      <td>0.006811</td>\n",
              "      <td>0.023625</td>\n",
              "      <td>0.009492</td>\n",
              "      <td>0.024048</td>\n",
              "      <td>0.009727</td>\n",
              "      <td>0.019922</td>\n",
              "      <td>0.032482</td>\n",
              "      <td>0.013387</td>\n",
              "      <td>0.004230</td>\n",
              "      <td>0.005016</td>\n",
              "      <td>0.029726</td>\n",
              "      <td>0.002253</td>\n",
              "      <td>0.023158</td>\n",
              "      <td>0.004487</td>\n",
              "      <td>0.039354</td>\n",
              "      <td>0.006132</td>\n",
              "      <td>0.038345</td>\n",
              "      <td>0.011391</td>\n",
              "      <td>0.014978</td>\n",
              "      <td>0.007929</td>\n",
              "      <td>0.028840</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.036366</td>\n",
              "      <td>0.009815</td>\n",
              "      <td>0.048014</td>\n",
              "      <td>0.007649</td>\n",
              "      <td>0.010921</td>\n",
              "      <td>0.005552</td>\n",
              "      <td>0.020476</td>\n",
              "      <td>0.023649</td>\n",
              "      <td>0.063418</td>\n",
              "      <td>0.013987</td>\n",
              "      <td>0.018404</td>\n",
              "      <td>0.005415</td>\n",
              "      <td>0.005418</td>\n",
              "      <td>0.007704</td>\n",
              "      <td>0.014829</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>0.060265</td>\n",
              "      <td>0.033150</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4709 rows Ã— 66 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Kingdom  Ncodons       UUU  ...       UAA       UAG       UGA\n",
              "0        rod     1139  0.043869  ...  0.000093  0.001189  0.029038\n",
              "1        vrt     1012  0.013118  ...  0.011095  0.004745  0.027767\n",
              "2        vrl     1422  0.022753  ...  0.004289  0.008880  0.004675\n",
              "3        bct  1150215  0.012533  ...  0.001758  0.000758  0.006248\n",
              "4        vrl     3673  0.035727  ...  0.008559  0.001079  0.005837\n",
              "...      ...      ...       ...  ...       ...       ...       ...\n",
              "4704     pln    16973  0.027968  ...  0.003303  0.015236  0.003655\n",
              "4705     bct     2224  0.044447  ...  0.014396  0.002363  0.014104\n",
              "4706     inv     1690  0.004434  ...  0.006460  0.000029  0.008474\n",
              "4707     pln     5723  0.045390  ...  0.005032  0.002087  0.021542\n",
              "4708     inv     1808  0.002657  ...       NaN       NaN       NaN\n",
              "\n",
              "[4709 rows x 66 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIT3yffd-PcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd79d169-02b2-46bc-fb48-8b93d1ebb52f"
      },
      "source": [
        "train = pd.read_csv(\"new_train.csv\")\n",
        "\n",
        "test = train[4500:5217]\n",
        "train = train[:4500]\n",
        "\n",
        "\n",
        "x_test=test\n",
        "\n",
        "y_train = train[[\"Kingdom_float\"]]\n",
        "x_train = train.drop(\"Kingdom\",axis=1)\n",
        "x_train = x_train.drop(\"Kingdom_float\", axis=1)\n",
        "\n",
        "if \"Kingdom_float\" in test.columns:\n",
        "  y_test = x_test[[\"Kingdom_float\"]]\n",
        "else:\n",
        "  y_test = []\n",
        "\n",
        "if \"Kingdom\" in x_test.columns:\n",
        "  x_test = x_test.drop(\"Kingdom\",axis=1)\n",
        "\n",
        "if \"Kingdom_float\" in x_test.columns:\n",
        "  x_test = x_test.drop(\"Kingdom_float\", axis=1)\n",
        "\n",
        "x_test_ids=[]\n",
        "\n",
        "if\"Id\" in x_test.columns:\n",
        "  test_id = x_test[[\"Id\"]]\n",
        "  x_test_ids = pd.DataFrame(test_id)\n",
        "  x_test = x_test.drop(\"Id\",axis=1)\n",
        "\n",
        "\n",
        "print(x_train)\n",
        "print(y_train)\n",
        "print(x_test)\n",
        "print(y_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Ncodons       UUU       UUC  ...       UAA       UAG       UGA\n",
            "0        1139  0.043869  0.035073  ...  0.000093  0.001189  0.029038\n",
            "1        1012  0.013118  0.057591  ...  0.011095  0.004745  0.027767\n",
            "2        1422  0.022753  0.024898  ...  0.004289  0.008880  0.004675\n",
            "3     1150215  0.012533  0.042499  ...  0.001758  0.000758  0.006248\n",
            "4        3673  0.035727  0.030558  ...  0.008559  0.001079  0.005837\n",
            "...       ...       ...       ...  ...       ...       ...       ...\n",
            "4495     1834  0.016830  0.026392  ...  0.003977  0.005823  0.011598\n",
            "4496     3583  0.023744  0.021545  ...  0.007098  0.002947  0.009474\n",
            "4497  1604841  0.038900  0.015344  ...  0.007987  0.005558  0.009680\n",
            "4498     1134  0.101509  0.009626  ...  0.010097  0.001761  0.029354\n",
            "4499     3769  0.031038  0.018839  ...  0.002757  0.006089  0.002928\n",
            "\n",
            "[4500 rows x 65 columns]\n",
            "      Kingdom_float\n",
            "0               0.0\n",
            "1               1.0\n",
            "2               2.0\n",
            "3               3.0\n",
            "4               2.0\n",
            "...             ...\n",
            "4495            3.0\n",
            "4496            3.0\n",
            "4497            3.0\n",
            "4498            5.0\n",
            "4499            3.0\n",
            "\n",
            "[4500 rows x 1 columns]\n",
            "      Ncodons       UUU       UUC  ...       UAA       UAG       UGA\n",
            "4500     3418  0.021796  0.045494  ...  0.004110  0.004142  0.007360\n",
            "4501     9674  0.022072  0.013566  ...  0.005586  0.008568  0.002253\n",
            "4502     2647  0.024837  0.027974  ...  0.003617  0.002286  0.003846\n",
            "4503     1330  0.023139  0.019596  ...  0.004297  0.000787  0.004622\n",
            "4504     1254  0.047095  0.025842  ...  0.007092  0.000692  0.003433\n",
            "...       ...       ...       ...  ...       ...       ...       ...\n",
            "5206    24233  0.035978  0.016597  ...  0.004132  0.005985  0.002121\n",
            "5207     8220  0.017681  0.056234  ...  0.003257  0.002345  0.027181\n",
            "5208     7936  0.035387  0.019296  ...  0.001247  0.002697  0.001905\n",
            "5209     2341  0.034190  0.015596  ...  0.001871  0.003318  0.009242\n",
            "5210     6361  0.045843  0.017095  ...  0.003558  0.002361  0.005433\n",
            "\n",
            "[711 rows x 65 columns]\n",
            "      Kingdom_float\n",
            "4500            8.0\n",
            "4501            3.0\n",
            "4502            1.0\n",
            "4503            3.0\n",
            "4504            2.0\n",
            "...             ...\n",
            "5206            3.0\n",
            "5207            1.0\n",
            "5208            2.0\n",
            "5209            2.0\n",
            "5210            4.0\n",
            "\n",
            "[711 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po1Sw32_-aOA"
      },
      "source": [
        "# **feature tuning (NOT USED FOR TRAINING MODEL)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVK1aKb6JV8k"
      },
      "source": [
        "**Attempt to remove the Ncodons (not done in the final model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cVB4n0fdzA_"
      },
      "source": [
        "\n",
        "x_train=x_train.drop(\"Ncodons\",axis=1)\n",
        "x_test=x_test.drop(\"Ncodons\",axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpUpw0Zb-dMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb497809-06ef-491a-97fb-504b631a67e7"
      },
      "source": [
        "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5000, \n",
        "                                        criterion ='entropy') \n",
        "\n",
        "X=x_train\n",
        "y=y_train \n",
        "\n",
        "# Training the model \n",
        "extra_tree_forest.fit(X, y) \n",
        "  \n",
        "# Computing the importance of each feature \n",
        "feature_importance = extra_tree_forest.feature_importances_ \n",
        "  \n",
        "# Normalizing the individual importances \n",
        "feature_importance_normalized = np.std([tree.feature_importances_ for tree in \n",
        "                                        extra_tree_forest.estimators_], \n",
        "                                        axis = 0) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "WmQYi5-fJkKA",
        "outputId": "1fbfc503-dac3-41e4-cf77-6327d91297a9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "X=x_train\n",
        "y=y_train\n",
        "k=40\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X,y)\n",
        "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
        "#plot graph of feature importances for better visualization\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "feat_importances.nlargest(k).plot(kind='barh')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.01594443 0.01669964 0.01579993 0.01391531 0.02015449 0.01623923\n",
            " 0.02957935 0.02017637 0.01789797 0.01737744 0.01829081 0.00984027\n",
            " 0.01117774 0.01070105 0.01183076 0.01281201 0.01341228 0.02092696\n",
            " 0.01334246 0.02268362 0.01116884 0.01395819 0.01540375 0.01662622\n",
            " 0.01261909 0.01486628 0.01751341 0.0132672  0.0109471  0.01639247\n",
            " 0.01307671 0.01162559 0.01061989 0.01110429 0.0108517  0.01356423\n",
            " 0.01718464 0.02047563 0.01187811 0.01618903 0.01082607 0.01244944\n",
            " 0.01574151 0.0122934  0.01136828 0.01993946 0.01592084 0.0129799\n",
            " 0.00969133 0.0218283  0.02103105 0.01267152 0.01781648 0.01244262\n",
            " 0.01391874 0.02473718 0.01997761 0.02664893 0.01756255 0.02225697\n",
            " 0.01712904 0.00839726 0.00682468 0.02741232]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtUhUkOolhw_",
        "outputId": "ce03ac9f-8af8-4158-b854-4ccfa4ff02d2"
      },
      "source": [
        "feature_importance_normalized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00283533, 0.00603003, 0.01084759, 0.00703118, 0.01065336,\n",
              "       0.0078099 , 0.01209803, 0.03577239, 0.01031999, 0.00776406,\n",
              "       0.01798567, 0.01153778, 0.00970819, 0.00646848, 0.00390976,\n",
              "       0.00387022, 0.00763528, 0.00412343, 0.01180857, 0.00435587,\n",
              "       0.0145907 , 0.00406165, 0.00873168, 0.01108966, 0.01287005,\n",
              "       0.01142646, 0.010011  , 0.01064938, 0.00467951, 0.00249418,\n",
              "       0.00741522, 0.00734829, 0.00702192, 0.00751026, 0.00865446,\n",
              "       0.00332557, 0.00599541, 0.01131936, 0.01467088, 0.00740699,\n",
              "       0.00645642, 0.00286552, 0.00394192, 0.01067754, 0.00847661,\n",
              "       0.00389805, 0.00874207, 0.00504181, 0.00563701, 0.0059812 ,\n",
              "       0.00886061, 0.0178755 , 0.00953654, 0.01305351, 0.00486934,\n",
              "       0.00555995, 0.01781131, 0.01239427, 0.02290955, 0.00745951,\n",
              "       0.01195809, 0.01627619, 0.00169471, 0.00146867, 0.03381902])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCcwHjXKl03v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "0a845313-a5eb-418c-abee-40e5484860f3"
      },
      "source": [
        "CUA    0.035661\n",
        "UGA    0.033519\n",
        "GAU    0.023655\n",
        "AGA    0.018560\n",
        "AUC    0.017727\n",
        "AAG    0.017362\n",
        "GAG    0.016186\n",
        "ACA    0.015141\n",
        "GCG    0.014000\n",
        "CGC    0.013083\n",
        "CCG    0.012799\n",
        "CUC    0.012719\n",
        "AGG    0.012273\n",
        "GCC    0.012272\n",
        "GAA    0.012227\n",
        "UGG    0.011690\n",
        "ACC    0.011451\n",
        "AUA    0.011349\n",
        "UUG    0.011002\n",
        "CAG    0.010861\n",
        "dtype: float64"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-59986283156b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    CUA    0.035661\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY8RUadCKOZn"
      },
      "source": [
        "**Not Included in final model, trained on all features since there was slight dip in accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "lx6PD0j5DZBX",
        "outputId": "cff4390e-f2ed-4305-e924-a0fb069ba074"
      },
      "source": [
        "col = feature_cols\n",
        "\n",
        "x_train = x_train[col]\n",
        "x_test = x_test[col]\n",
        "x_train.shape\n",
        "x_test.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-0209a7651a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'feature_cols' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sghgvGZKKd60"
      },
      "source": [
        "# **Feature Scaling and Imbalance class handling using SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIedww6XKeNv",
        "outputId": "8501d467-2c34-4e80-b74f-2cecf53a9180"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "feature_scaler = StandardScaler()\n",
        "x_train = feature_scaler.fit_transform(x_train)\n",
        "x_test = feature_scaler.transform(x_test)\n",
        "\n",
        "print(x_train)\n",
        "print(x_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.09228695  0.83248594  0.62616395 ... -1.60260706 -1.07925236\n",
            "   1.76935798]\n",
            " [-0.09242528 -0.86899891  2.49320936 ...  1.59138538  0.0691102\n",
            "   1.65055148]\n",
            " [-0.09197869 -0.33586855 -0.21748849 ... -0.38444     1.4045098\n",
            "  -0.50834955]\n",
            " ...\n",
            " [ 1.65455282  0.55755261 -1.00965623 ...  0.68893454  0.33173179\n",
            "  -0.0404584 ]\n",
            " [-0.09229239  4.02176643 -1.48380341 ...  1.30147596 -0.89468986\n",
            "   1.79888338]\n",
            " [-0.08942221  0.12254656 -0.71990216 ... -0.82919319  0.5033134\n",
            "  -0.67171493]]\n",
            "[[-0.08980454 -0.38883047  1.4901448  ... -0.43651599 -0.12557691\n",
            "  -0.25737203]\n",
            " [-0.08299016 -0.37354245 -1.15707693 ... -0.00802331  1.30371541\n",
            "  -0.73484702]\n",
            " [-0.09064435 -0.22055049  0.03755384 ... -0.57955701 -0.72502858\n",
            "  -0.5859053 ]\n",
            " ...\n",
            " [-0.08488328  0.3631574  -0.68201828 ... -1.26770181 -0.59237615\n",
            "  -0.76734372]\n",
            " [-0.09097766  0.29692601 -0.98879839 ... -1.08657874 -0.39189898\n",
            "  -0.0814014 ]\n",
            " [-0.08659886  0.94173064 -0.86450429 ... -0.59678025 -0.70094683\n",
            "  -0.43753021]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5VJK7GAc5YA",
        "outputId": "e3cfaf90-584a-4d7a-a3e8-36d19cbca05e"
      },
      "source": [
        "pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN0IDFX-ce9z",
        "outputId": "1a1419a5-1db3-4f1d-e266-9cf9e87388c5"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "x_train, y_train = oversample.fit_resample(x_train, y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TiBHDnwVSgE",
        "outputId": "2ee7312b-321e-4152-b540-f181ca97982f"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10934, 64)\n",
            "(10934,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4GffCzhLCQG"
      },
      "source": [
        "# **Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIlfvxRo-m5d"
      },
      "source": [
        "**different algorithms params**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4M_EQYQ-_xa"
      },
      "source": [
        "\n",
        "#xg_boost\n",
        "xg_p = {'max_depth': [100],  \n",
        "              'alpha' : [0.01,0.1],\n",
        "              'learning_rate' : [0.01],\n",
        "              'n_estimators' : [200],\n",
        "              'colsample_bytree':[0.3],\n",
        "              'nthread': [-1]} \n",
        "# not used\n",
        "svc_p = {\n",
        "    'kernel': ('linear','poly','rbf','sigmoid'),\n",
        "    'decision_function_shape': ('ovo','ovr'),\n",
        "    'C': (0.1,0.5, 2.0,5.0,10.0)\n",
        "  } \n",
        "\n",
        "# Extra Trees Parameters\n",
        "et_p = {\n",
        "\n",
        "    'n_jobs': [-1],\n",
        "    'n_estimators':[200],\n",
        "    'max_features':['auto','sqrt'],\n",
        "    'class_weight':['balanced_subsample']\n",
        "}\n",
        "\n",
        "\n",
        "# Random Forest parameters\n",
        "rf_p = {\n",
        "  \n",
        "'n_jobs': [-1],\n",
        "    'n_estimators': [500],\n",
        "    'max_depth': [500],\n",
        "     'max_features':['sqrt']\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMAv4_pETNrC",
        "outputId": "5e761087-81d1-447a-e784-8ec6554b268c"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10934, 65)\n",
            "(711, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rolnZKSlMsun"
      },
      "source": [
        "**Tried Extra Tree , Random Forest and SVM\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "great)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGe4RZ_JRKy_"
      },
      "source": [
        "**Extra Trees Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVP95GPaNRcQ",
        "outputId": "7bd5bad4-0b11-4a0d-84be-add10256687f"
      },
      "source": [
        "# print('training model Extra Trees')\n",
        "grid_clf = GridSearchCV(ExtraTreesClassifier()\n",
        "      ,et_p, refit = True, verbose = 3,cv=StratifiedKFold(n_splits=2, \n",
        "                        random_state=42).split(x_train, y_train)) \n",
        "\n",
        "grid_clf.fit(x_train, y_train)\n",
        "y_preds = grid_clf.predict(x_test)\n",
        "report = classification_report(y_test, y_preds)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "[CV] class_weight=balanced_subsample, max_features=auto, n_estimators=200, n_jobs=-1 \n",
            "[CV]  class_weight=balanced_subsample, max_features=auto, n_estimators=200, n_jobs=-1, score=0.943, total=   2.1s\n",
            "[CV] class_weight=balanced_subsample, max_features=auto, n_estimators=200, n_jobs=-1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  class_weight=balanced_subsample, max_features=auto, n_estimators=200, n_jobs=-1, score=0.957, total=   2.2s\n",
            "[CV] class_weight=balanced_subsample, max_features=sqrt, n_estimators=200, n_jobs=-1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  class_weight=balanced_subsample, max_features=sqrt, n_estimators=200, n_jobs=-1, score=0.940, total=   2.0s\n",
            "[CV] class_weight=balanced_subsample, max_features=sqrt, n_estimators=200, n_jobs=-1 \n",
            "[CV]  class_weight=balanced_subsample, max_features=sqrt, n_estimators=200, n_jobs=-1, score=0.957, total=   2.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    8.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.25      0.38        12\n",
            "         1.0       0.93      0.87      0.90       104\n",
            "         2.0       0.78      0.91      0.84       155\n",
            "         3.0       0.87      0.93      0.90       174\n",
            "         4.0       0.81      0.81      0.81       129\n",
            "         5.0       0.82      0.62      0.71        76\n",
            "         6.0       0.71      0.80      0.75        30\n",
            "         7.0       0.91      0.71      0.80        14\n",
            "         8.0       0.57      0.44      0.50         9\n",
            "         9.0       1.00      0.50      0.67         8\n",
            "\n",
            "    accuracy                           0.83       711\n",
            "   macro avg       0.81      0.68      0.72       711\n",
            "weighted avg       0.83      0.83      0.82       711\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1cgHPnoRSwh"
      },
      "source": [
        "**Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoptmEoxRDSi",
        "outputId": "b630fa56-fe7a-407c-b8aa-9d196ecda3b0"
      },
      "source": [
        "# print('training model Extra Trees')\n",
        "grid_clf = GridSearchCV(RandomForestClassifier()\n",
        "      ,rf_p, refit = True, verbose = 3,cv=StratifiedKFold(n_splits=2, \n",
        "                        random_state=42).split(x_train, y_train)) \n",
        "\n",
        "grid_clf.fit(x_train, y_train)\n",
        "y_rf_preds = grid_clf.predict(x_test)\n",
        "report = classification_report(y_test, y_rf_preds)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] max_depth=500, max_features=sqrt, n_estimators=500, n_jobs=-1 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=500, max_features=sqrt, n_estimators=500, n_jobs=-1, score=0.935, total=  16.5s\n",
            "[CV] max_depth=500, max_features=sqrt, n_estimators=500, n_jobs=-1 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=500, max_features=sqrt, n_estimators=500, n_jobs=-1, score=0.955, total=  15.9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   32.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   32.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.17      0.25        12\n",
            "         1.0       0.94      0.88      0.91       104\n",
            "         2.0       0.80      0.90      0.85       155\n",
            "         3.0       0.85      0.91      0.88       174\n",
            "         4.0       0.83      0.88      0.86       129\n",
            "         5.0       0.84      0.63      0.72        76\n",
            "         6.0       0.75      0.80      0.77        30\n",
            "         7.0       0.73      0.57      0.64        14\n",
            "         8.0       0.62      0.56      0.59         9\n",
            "         9.0       0.80      0.50      0.62         8\n",
            "\n",
            "    accuracy                           0.84       711\n",
            "   macro avg       0.77      0.68      0.71       711\n",
            "weighted avg       0.83      0.84      0.83       711\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi__9RHBRahm"
      },
      "source": [
        "**Xg_boost Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dhaV_W7RZ_d",
        "outputId": "6463321c-2d77-4084-ebc6-e9ac9afbd4e1"
      },
      "source": [
        "grid_clf = GridSearchCV(xgb.XGBClassifier(objective=\"multi:softprob\",subsample=0.5,nthread= -1)   \n",
        "      ,xg_p, refit = True, verbose = 3,cv=StratifiedKFold(n_splits=2, \n",
        "                        random_state=42).split(x_train, y_train)) \n",
        "\n",
        "grid_clf.fit(x_train, y_train)\n",
        "y_xg_preds = grid_clf.predict(x_test)\n",
        "report = classification_report(y_test, y_xg_preds)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "[CV] alpha=0.01, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1 \n",
            "[CV]  alpha=0.01, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1, score=0.917, total=  35.3s\n",
            "[CV] alpha=0.01, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  alpha=0.01, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1, score=0.942, total=  36.0s\n",
            "[CV] alpha=0.1, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  alpha=0.1, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1, score=0.917, total=  35.2s\n",
            "[CV] alpha=0.1, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1 \n",
            "[CV]  alpha=0.1, colsample_bytree=0.3, learning_rate=0.01, max_depth=100, n_estimators=200, nthread=-1, score=0.942, total=  36.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.17      0.25        12\n",
            "         1.0       0.93      0.86      0.89       104\n",
            "         2.0       0.81      0.90      0.85       155\n",
            "         3.0       0.87      0.89      0.88       174\n",
            "         4.0       0.85      0.87      0.86       129\n",
            "         5.0       0.80      0.70      0.75        76\n",
            "         6.0       0.74      0.77      0.75        30\n",
            "         7.0       0.69      0.79      0.73        14\n",
            "         8.0       0.56      0.56      0.56         9\n",
            "         9.0       0.67      0.50      0.57         8\n",
            "\n",
            "    accuracy                           0.83       711\n",
            "   macro avg       0.74      0.70      0.71       711\n",
            "weighted avg       0.83      0.83      0.83       711\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85ytWGL9MlWn"
      },
      "source": [
        "**logistic regression classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfC_67xGMknW",
        "outputId": "99732d6d-8063-42b9-d9a7-b119a8e59f9b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "p = {'multi_class':['auto'],\n",
        "     'penalty': ['l2'],\n",
        "     'class_weight':['balanced'],\n",
        "     #'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "     'solver' : ['lbfgs'],\n",
        "     'max_iter': [1000],\n",
        "     'C':[100]\n",
        "     }\n",
        "\n",
        "grid_clf = GridSearchCV(\n",
        "    LogisticRegression(),\n",
        "    p,\n",
        "    cv=StratifiedKFold(n_splits=2, \n",
        "                        random_state=42,shuffle=True).split(x_train, y_train))\n",
        "\n",
        "grid_clf.fit(x_train, y_train)\n",
        "y_xg_preds = grid_clf.predict(x_test)\n",
        "report = classification_report(y_test, y_xg_preds)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.17      0.33      0.23        12\n",
            "         1.0       0.90      0.78      0.84       104\n",
            "         2.0       0.83      0.85      0.84       155\n",
            "         3.0       0.88      0.75      0.81       174\n",
            "         4.0       0.79      0.64      0.71       129\n",
            "         5.0       0.47      0.54      0.50        76\n",
            "         6.0       0.62      0.60      0.61        30\n",
            "         7.0       0.27      0.43      0.33        14\n",
            "         8.0       0.28      0.78      0.41         9\n",
            "         9.0       0.42      0.62      0.50         8\n",
            "        10.0       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.71       711\n",
            "   macro avg       0.51      0.58      0.53       711\n",
            "weighted avg       0.77      0.71      0.73       711\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDl8aW4AQ5i6"
      },
      "source": [
        "**SVC trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3aM7-3WQ3h-",
        "outputId": "feffe964-ca53-43eb-c769-eb2b1b1f836a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        " \n",
        "# defining parameter range \n",
        " \n",
        "# grid_lr= make_pipeline(StandardScaler(),\n",
        "#  SVC(kernel='rbf',tol=0.0001,C=100,max_iter=3000))\n",
        " \n",
        "p = {\n",
        "    'kernel':['rbf'],\n",
        "     'tol': [0.00001,0.000001,0.000001],\n",
        "     'C': [1,10,100],\n",
        "     'max_iter': [3000],\n",
        "     'decision_function_shape': ['ovo'],\n",
        "     'gamma':['auto'],\n",
        "     #'loss':['hinge']\n",
        "     'class_weight': [ {\n",
        "         0:5,\n",
        "         1:0.25,\n",
        "         2:0.25,\n",
        "         3:0.25,\n",
        "         4:0.25,\n",
        "         5:0.5,\n",
        "         6:0.5,\n",
        "         7:5,\n",
        "         8:5,\n",
        "         9:5} ]\n",
        "     }\n",
        "\n",
        "grid_clf = GridSearchCV(\n",
        "    SVC(),\n",
        "    p,\n",
        "    cv=StratifiedKFold(n_splits=2, \n",
        "                        random_state=42,shuffle=True).split(x_train, y_train))\n",
        "\n",
        "grid_clf.fit(x_train, y_train)\n",
        "final_model=grid_clf\n",
        "y_xg_preds = grid_clf.predict(x_test)\n",
        "report = classification_report(y_test, y_xg_preds)\n",
        "print(report)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.33      0.42        12\n",
            "         1.0       0.93      0.95      0.94       104\n",
            "         2.0       0.89      0.91      0.90       155\n",
            "         3.0       0.91      0.93      0.92       174\n",
            "         4.0       0.87      0.88      0.87       129\n",
            "         5.0       0.82      0.82      0.82        76\n",
            "         6.0       0.83      0.80      0.81        30\n",
            "         7.0       0.62      0.57      0.59        14\n",
            "         8.0       0.75      0.67      0.71         9\n",
            "         9.0       1.00      0.62      0.77         8\n",
            "\n",
            "    accuracy                           0.88       711\n",
            "   macro avg       0.82      0.75      0.78       711\n",
            "weighted avg       0.87      0.88      0.88       711\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "SvqONWUHS111",
        "outputId": "f07c5846-91d8-4b5e-9682-7764cfa8848a"
      },
      "source": [
        "base_predictions_train = pd.DataFrame( {\n",
        "     \n",
        "     'RandomForest': y_rf_preds,\n",
        "     'ExtraTrees': y_preds,\n",
        "     'xgBoost' : y_xg_preds\n",
        "    })\n",
        "base_predictions_train.head(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>ExtraTrees</th>\n",
              "      <th>xgBoost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    RandomForest  ExtraTrees  xgBoost\n",
              "0            8.0         8.0      8.0\n",
              "1            3.0         3.0      3.0\n",
              "2            2.0         2.0      2.0\n",
              "3            2.0         2.0      2.0\n",
              "4            2.0         2.0      2.0\n",
              "..           ...         ...      ...\n",
              "95           2.0         2.0      2.0\n",
              "96           1.0         1.0      1.0\n",
              "97           3.0         3.0      3.0\n",
              "98           4.0         4.0      4.0\n",
              "99           4.0         2.0      4.0\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X9YTa8eTHbl",
        "outputId": "84cb72df-e7a4-465d-f021-b09677e1c04a"
      },
      "source": [
        "base_predictions_train['max_freq'] = base_predictions_train.agg(lambda x: x.mode() if x.mode().size==1 else x['xgBoost'], axis=1)\n",
        "y_preds = base_predictions_train['max_freq']\n",
        "print(y_preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0      8.0\n",
            "1      3.0\n",
            "2      2.0\n",
            "3      2.0\n",
            "4      2.0\n",
            "      ... \n",
            "706    3.0\n",
            "707    1.0\n",
            "708    2.0\n",
            "709    2.0\n",
            "710    2.0\n",
            "Name: max_freq, Length: 711, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIzj-PfDLLAW"
      },
      "source": [
        "# **Save result**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vHK6wN4dd0_"
      },
      "source": [
        "# **standardize features for svm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQfIB8hrdYZW"
      },
      "source": [
        "def standardize(x_train,x_test):\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  feature_scaler = StandardScaler()\n",
        "  x_train = feature_scaler.fit_transform(x_train)\n",
        "  x_test = feature_scaler.transform(x_test)\n",
        "  return x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfitAAMehj14"
      },
      "source": [
        "def save_test_results(y_preds,test,filename):\n",
        "\n",
        "    test_id = test[\"Id\"]\n",
        "\n",
        "    df = pd.concat([pd.DataFrame(test_id),pd.DataFrame(y_preds)],axis=1)\n",
        "\n",
        "    \n",
        "    df = df.rename(columns={0: 'Kingdom'})\n",
        "    pd.set_option('display.max_rows', df.shape[0]+1)\n",
        "    print(df.head(300))\n",
        "\n",
        "    df.to_csv(filename,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcY-WyHthNqd"
      },
      "source": [
        "def get_kingdom_dict():\n",
        "\n",
        "    kingdom=dict()\n",
        "\n",
        "    with open('kingdom_lookup.txt') as f:\n",
        "        val = f.readlines()\n",
        "\n",
        "        for v in val:\n",
        "\n",
        "            key_pair = v.split(',')\n",
        "            val= str(key_pair[0].strip())\n",
        "            key = float(key_pair[1].strip())\n",
        "\n",
        "            kingdom[val]=key\n",
        "\n",
        "    return kingdom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtWy3Cy9vfVD"
      },
      "source": [
        "\n",
        "def convert_preds_float_to_str(preds):\n",
        "\n",
        "    kingdom = get_kingdom_dict()\n",
        "    rev_kingdom_dic =dict()\n",
        "\n",
        "    for k,v in kingdom.items():\n",
        "\n",
        "        rev_kingdom_dic[v]=k\n",
        "\n",
        "    print(preds.shape)\n",
        "    new_preds = np.empty(preds.shape,dtype=object)\n",
        "\n",
        "    print(new_preds.size)\n",
        "\n",
        "    for i, val in enumerate(preds):\n",
        "        if val in rev_kingdom_dic:\n",
        "            new_preds[i] = rev_kingdom_dic[val]\n",
        "\n",
        "    print(new_preds)\n",
        "\n",
        "    return new_preds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYiM0vVFheeN"
      },
      "source": [
        "\n",
        "def convert_kingdom_to_float(label):\n",
        "\n",
        "    kingdom = get_kingdom_dict()\n",
        "\n",
        "    for i,val in enumerate(label.values):\n",
        "        if val[0] in kingdom:\n",
        "            val[0] = kingdom[val[0]]\n",
        "\n",
        "    return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nML4TmJig5jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99186547-f8c4-48bc-9182-d69660e48c38"
      },
      "source": [
        "\n",
        "test = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "#f_test=test.drop(\"Ncodons\",axis=1)\n",
        "f_test=test.drop(\"Id\",axis=1)\n",
        "print(f_test.head(10))\n",
        "x_train=x_train\n",
        "f_test=feature_scaler.transform(f_test)\n",
        "f_test=standardize(x_train,f_test)\n",
        "y_preds = final_model.predict(f_test)\n",
        "print(y_preds)\n",
        "y_preds = convert_preds_float_to_str(y_preds)\n",
        "\n",
        "\n",
        "\n",
        "save_test_results(y_preds,test,\"result.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-328949970ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#f_test=test.drop(\"Ncodons\",axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLrCZ3gCe9LO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}